# 参数优化方法详细说明文档

## 目录
1. [优化器概述](#优化器概述)
2. [参数评估机制](#参数评估机制)
3. [网格搜索（Grid Search）](#网格搜索grid-search)
4. [随机搜索（Random Search）](#随机搜索random-search)
5. [贝叶斯优化（Bayesian Optimization）](#贝叶斯优化bayesian-optimization)
6. [过拟合检测机制](#过拟合检测机制)
7. [三种方法对比](#三种方法对比)

---

## 优化器概述

### 优化目标

参数优化器用于寻找使交易策略表现最佳的参数组合。支持三种优化目标：

1. **夏普比率（sharpe_ratio）**：风险调整后的收益率
2. **总收益率（return）**：绝对收益率
3. **收益率/回撤比（return_drawdown_ratio）**：收益率与最大回撤的比值

### 参数搜索空间

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
self.param_space = {
    'lookback_period': {'type': 'int', 'coarse': [30, 60, 90, 120], 'fine_step': 10},
    'z_threshold': {'type': 'float', 'coarse': [1.0, 1.5, 2.0, 2.5, 3.0], 'fine_step': 0.1},
    'z_exit_threshold': {'type': 'float', 'coarse': [0.3, 0.5, 0.7, 0.9], 'fine_step': 0.1},
    'take_profit_pct': {'type': 'float', 'coarse': [0.05, 0.10, 0.15, 0.20, 0.25], 'fine_step': 0.02},
    'stop_loss_pct': {'type': 'float', 'coarse': [0.05, 0.08, 0.10, 0.12, 0.15], 'fine_step': 0.01},
}
```

**参数说明**：
- `coarse`：粗粒度搜索的候选值列表
- `fine_step`：细粒度搜索的步长
- `type`：参数类型（'int' 或 'float'）

---

## 参数评估机制

### evaluate_params 方法

所有优化方法都使用相同的参数评估机制：

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
def evaluate_params(self, params: Dict[str, Any], verbose: bool = False) -> Dict[str, Any]:
    """
    评估参数组合
    
    Args:
        params: 参数字典
        verbose: 是否打印详细信息
        
    Returns:
        评估结果字典
    """
    try:
        # 创建策略实例（使用策略对象）
        strategy = AdvancedCointegrationTrading(
            lookback_period=params['lookback_period'],
            z_threshold=params['z_threshold'],
            z_exit_threshold=params['z_exit_threshold'],
            take_profit_pct=params['take_profit_pct'],
            stop_loss_pct=params['stop_loss_pct'],
            max_holding_hours=params.get('max_holding_hours', 168),
            position_ratio=params.get('position_ratio', 0.5),
            leverage=params.get('leverage', 5),
            trading_fee_rate=params.get('trading_fee_rate', 0.000275),
            z_score_strategy=self.z_score_strategy
        )
        
        # 执行回测（不显示图表和详细输出）
        strategy._optimization_mode = True
        results = strategy.backtest_cointegration_trading(
            self.data,
            self.selected_pairs,
            initial_capital=self.initial_capital
        )
        
        # 计算风险指标
        risk_metrics = strategy.calculate_risk_metrics(results['capital_curve'])
        
        # 计算最终收益
        final_capital = results['capital_curve'][-1]['capital'] if results['capital_curve'] else self.initial_capital
        total_return = (final_capital - self.initial_capital) / self.initial_capital
        
        # 根据目标函数计算得分
        if self.objective == 'sharpe_ratio':
            score = risk_metrics.get('sharpe_ratio', 0)
        elif self.objective == 'return':
            score = total_return * 100
        elif self.objective == 'return_drawdown_ratio':
            max_dd = risk_metrics.get('max_drawdown_pct', 1)
            score = (total_return * 100) / max_dd if max_dd > 0 else 0
        else:
            score = risk_metrics.get('sharpe_ratio', 0)
        
        evaluation_result = {
            'params': params.copy(),
            'score': score,
            'total_return': total_return * 100,
            'sharpe_ratio': risk_metrics.get('sharpe_ratio', 0),
            'max_drawdown_pct': risk_metrics.get('max_drawdown_pct', 0),
            'profit_loss_ratio': risk_metrics.get('profit_loss_ratio', 0),
            'total_trades': risk_metrics.get('total_trades', 0),
            'profitable_trades': risk_metrics.get('profitable_trades', 0),
            'final_capital': final_capital
        }
        
        # 记录评估历史
        self.evaluation_history.append(evaluation_result)
        
        return evaluation_result
```

**评估流程**：

```
输入参数组合
  ↓
创建交易策略实例（使用这些参数）
  ↓
执行回测（不显示图表）
  ↓
计算风险指标（夏普比率、最大回撤等）
  ↓
根据优化目标计算得分
  ↓
返回评估结果（包含得分和所有指标）
```

**得分计算**：

- **sharpe_ratio**：直接使用夏普比率作为得分
- **return**：总收益率 × 100
- **return_drawdown_ratio**：(总收益率 × 100) / 最大回撤百分比

---

## 网格搜索（Grid Search）

### 原理

网格搜索是**穷举搜索**方法，系统地遍历参数空间中的所有可能组合。

### 特点

- **优点**：全面搜索，不会遗漏最优解
- **缺点**：计算量大，参数空间大时不可行
- **适用场景**：参数空间较小，或需要全面搜索时

### 分层搜索策略

本系统采用**两层搜索**策略：

1. **第一阶段：粗粒度搜索**
   - 使用 `coarse` 列表中的值
   - 快速找到大致的最优区域

2. **第二阶段：细粒度搜索**
   - 在最佳参数附近进行精细搜索
   - 使用 `fine_step` 步长

### 详细步骤

#### 步骤1：生成粗粒度参数组合

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
# 生成所有粗粒度参数组合
param_names = list(self.param_space.keys())
coarse_values = [self.param_space[name]['coarse'] for name in param_names]
coarse_combinations = list(itertools.product(*coarse_values))
```

**原理**：
- 使用 `itertools.product` 生成所有参数的笛卡尔积
- 例如：如果有3个参数，每个参数有4个候选值，则生成 4×4×4 = 64 个组合

**示例**：
```
参数1: [30, 60, 90, 120]
参数2: [1.0, 1.5, 2.0, 2.5]
参数3: [0.3, 0.5, 0.7, 0.9]

组合数 = 4 × 4 × 4 = 64 个组合
```

#### 步骤2：限制组合数量

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
# 限制组合数量
if len(coarse_combinations) > max_coarse_combinations:
    print(f"粗粒度组合数过多({len(coarse_combinations)})，随机采样{max_coarse_combinations}个")
    coarse_combinations = random.sample(coarse_combinations, max_coarse_combinations)
```

**原理**：
- 如果组合数过多（超过 `max_coarse_combinations`，默认100），随机采样
- 避免计算时间过长

#### 步骤3：评估所有粗粒度组合

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
for i, combination in enumerate(coarse_combinations):
    params = dict(zip(param_names, combination))
    result = self.evaluate_params(params, verbose=(i % 10 == 0))
    
    if result['score'] > best_score:
        best_score = result['score']
        best_result = result
        print(f"   找到更好的参数组合 #{i+1}: 得分={best_score:.4f}")
```

**原理**：
- 遍历每个参数组合
- 调用 `evaluate_params` 评估该组合
- 记录最佳得分和对应的参数

#### 步骤4：生成细粒度搜索范围

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
# 为每个参数生成细粒度搜索范围
for param_name in param_names:
    space_def = self.param_space[param_name]
    base_value = best_params[param_name]  # 粗粒度搜索的最佳值
    fine_step = space_def['fine_step']
    
    if space_def['type'] == 'int':
        # 整数：在基础值附近生成几个值
        fine_values = [base_value + i * fine_step for i in range(-2, 3)]
        fine_values = [v for v in fine_values if v > 0]
    else:
        # 浮点：在基础值附近生成几个值
        fine_values = [base_value + i * fine_step for i in range(-2, 3)]
        fine_values = [max(0.01, v) for v in fine_values]
    
    fine_combinations.append(fine_values)
```

**原理**：
- 以粗粒度搜索的最佳值为中心
- 在前后各2个步长范围内生成细粒度值
- 例如：最佳值=60，步长=10，则生成 [40, 50, 60, 70, 80]

**示例**：
```
粗粒度最佳值: lookback_period=60, z_threshold=1.5
细粒度范围:
  lookback_period: [40, 50, 60, 70, 80]  (60 ± 2×10)
  z_threshold: [1.3, 1.4, 1.5, 1.6, 1.7]  (1.5 ± 2×0.1)
```

#### 步骤5：评估细粒度组合

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
# 生成细粒度组合（限制数量）
fine_product = list(itertools.product(*fine_combinations))
if len(fine_product) > 200:
    fine_product = random.sample(fine_product, 200)

print(f"测试 {len(fine_product)} 个细粒度参数组合...")

for i, combination in enumerate(fine_product):
    params = dict(zip(param_names, combination))
    result = self.evaluate_params(params, verbose=(i % 20 == 0))
    
    if result['score'] > best_score:
        best_score = result['score']
        best_result = result
```

**原理**：
- 生成细粒度参数组合
- 如果组合数超过200，随机采样200个
- 评估所有细粒度组合，更新最佳结果

### 完整流程图

```
开始网格搜索
  ↓
[第一阶段：粗粒度搜索]
生成所有粗粒度参数组合
  ↓
组合数 > 100？ → 是 → 随机采样100个
  ↓ 否
评估所有粗粒度组合
  ↓
记录最佳参数和得分
  ↓
[第二阶段：细粒度搜索]
在最佳参数附近生成细粒度范围
  ↓
生成细粒度参数组合
  ↓
组合数 > 200？ → 是 → 随机采样200个
  ↓ 否
评估所有细粒度组合
  ↓
更新最佳参数和得分
  ↓
[稳定性测试]（可选）
  ↓
返回最佳结果
```

---

## 随机搜索（Random Search）

### 原理

随机搜索是**随机采样**方法，在参数空间中随机选择参数组合进行评估。

### 特点

- **优点**：计算量可控，适合高维参数空间
- **缺点**：可能遗漏最优解，结果具有随机性
- **适用场景**：参数空间较大，计算资源有限时

### 详细步骤

#### 步骤1：随机生成参数

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
for i in range(n_iter):
    # 随机生成参数
    params = {}
    for param_name, space_def in self.param_space.items():
        if space_def['type'] == 'int':
            # 整数：从粗粒度范围中随机选择，然后添加随机扰动
            base = random.choice(space_def['coarse'])
            step = space_def['fine_step']
            params[param_name] = base + random.randint(-int(step), int(step))
            # 确保在合理范围
            if param_name == 'lookback_period':
                params[param_name] = max(10, min(200, params[param_name]))
        else:
            # 浮点：从粗粒度范围中随机选择，然后添加随机扰动
            base = random.choice(space_def['coarse'])
            step = space_def['fine_step']
            params[param_name] = base + random.uniform(-step, step)
            # 确保在合理范围
            if param_name in ['z_threshold', 'z_exit_threshold']:
                params[param_name] = max(0.1, min(5.0, params[param_name]))
```

**原理**：

1. **基础值选择**：
   - 从 `coarse` 列表中随机选择一个值作为基础

2. **添加扰动**：
   - 在基础值附近添加随机扰动
   - 整数参数：`base ± random.randint(-step, step)`
   - 浮点参数：`base ± random.uniform(-step, step)`

3. **边界检查**：
   - 确保参数在合理范围内

**示例**：
```
迭代1:
  base = random.choice([30, 60, 90, 120]) = 60
  step = 10
  lookback_period = 60 + random.randint(-10, 10) = 65

迭代2:
  base = random.choice([30, 60, 90, 120]) = 90
  step = 10
  lookback_period = 90 + random.randint(-10, 10) = 85
```

#### 步骤2：评估参数组合

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
result = self.evaluate_params(params, verbose=(i % 10 == 0))

if (i + 1) % 10 == 0 or i == n_iter - 1:
    print(f"  进度: {i+1}/{n_iter} ({100*(i+1)/n_iter:.1f}%), "
          f"当前最佳得分: {best_score:.4f}")

if result['score'] > best_score:
    best_score = result['score']
    best_result = result
    print(f"   迭代 {i+1}/{n_iter}: 找到更好的参数，得分={best_score:.4f}")
```

**原理**：
- 每次迭代生成一个随机参数组合
- 评估该组合
- 如果得分更好，更新最佳结果

### 完整流程图

```
开始随机搜索
  ↓
设置迭代次数 n_iter
  ↓
[循环 n_iter 次]
  ↓
随机生成参数组合
  ├─ 从coarse列表随机选择基础值
  ├─ 添加随机扰动
  └─ 边界检查
  ↓
评估参数组合
  ↓
得分更好？ → 是 → 更新最佳结果
  ↓ 否
继续下一次迭代
  ↓
[稳定性测试]（可选）
  ↓
返回最佳结果
```

---

## 贝叶斯优化（Bayesian Optimization）

### 原理

贝叶斯优化是**智能搜索**方法，使用概率模型（高斯过程）来指导参数选择。

### 核心思想

1. **建立概率模型**：使用已评估的参数组合建立目标函数的概率模型
2. **选择下一个评估点**：根据模型预测，选择最有希望找到最优解的参数组合
3. **更新模型**：评估新参数后，更新概率模型
4. **迭代优化**：重复步骤2-3，直到达到评估次数上限

### 特点

- **优点**：通常比随机搜索更高效，能找到更好的解
- **缺点**：需要额外的库（scikit-optimize），计算复杂度较高
- **适用场景**：参数空间较大，评估成本高时

### 详细步骤

#### 步骤1：定义搜索空间

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
# 定义搜索空间
dimensions = []
param_names = list(self.param_space.keys())

for param_name in param_names:
    space_def = self.param_space[param_name]
    if space_def['type'] == 'int':
        min_val = min(space_def['coarse'])
        max_val = max(space_def['coarse'])
        dimensions.append(Integer(min_val, max_val, name=param_name))
    else:
        min_val = min(space_def['coarse'])
        max_val = max(space_def['coarse'])
        dimensions.append(Real(min_val, max_val, name=param_name))
```

**原理**：
- 为每个参数定义搜索范围
- 整数参数使用 `Integer(min_val, max_val)`
- 浮点参数使用 `Real(min_val, max_val)`
- 范围基于 `coarse` 列表的最小值和最大值

**示例**：
```
lookback_period: Integer(30, 120)  # 从coarse列表 [30, 60, 90, 120] 得到
z_threshold: Real(1.0, 3.0)        # 从coarse列表 [1.0, 1.5, 2.0, 2.5, 3.0] 得到
```

#### 步骤2：定义目标函数

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
# 定义目标函数
@use_named_args(dimensions=dimensions)
def objective(**params):
    # 确保整数参数为整数
    for param_name, param_value in params.items():
        if param_name in self.param_space:
            space_def = self.param_space[param_name]
            if space_def['type'] == 'int':
                params[param_name] = int(round(param_value))
    
    result = self.evaluate_params(params, verbose=False)
    return -result['score']  # 最小化负得分（即最大化得分）
```

**原理**：
- 目标函数接收参数并返回得分
- 由于 `gp_minimize` 是最小化函数，返回负得分以实现最大化
- 确保整数参数被正确转换为整数

#### 步骤3：执行贝叶斯优化

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
# 执行贝叶斯优化
result_bo = gp_minimize(
    func=objective,
    dimensions=dimensions,
    n_calls=n_calls,
    random_state=42,
    acq_func='EI'  # Expected Improvement
)
```

**原理**：

1. **高斯过程（Gaussian Process）**：
   - 建立目标函数的概率模型
   - 预测未评估点的得分和不确定性

2. **采集函数（Acquisition Function）**：
   - `acq_func='EI'`：期望改进（Expected Improvement）
   - 选择最有希望改进当前最佳得分的参数组合

3. **优化过程**：
   - 初始随机采样几个点
   - 建立初始模型
   - 迭代：选择下一个评估点 → 评估 → 更新模型
   - 重复 n_calls 次

**示例流程**：
```
迭代1: 随机选择参数 → 评估 → 建立初始模型
迭代2: 模型预测 → 选择最有希望的点 → 评估 → 更新模型
迭代3: 模型预测 → 选择最有希望的点 → 评估 → 更新模型
...
迭代n_calls: 模型预测 → 选择最有希望的点 → 评估 → 更新模型
```

#### 步骤4：提取最佳参数

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
# 提取最佳参数并确保整数参数为整数
best_params = {}
for i, param_name in enumerate(param_names):
    param_value = result_bo.x[i]
    space_def = self.param_space[param_name]
    if space_def['type'] == 'int':
        best_params[param_name] = int(round(param_value))
        # 确保在合理范围内
        if param_name == 'lookback_period':
            best_params[param_name] = max(10, min(200, best_params[param_name]))
    else:
        best_params[param_name] = float(param_value)
        # 确保在合理范围内
        if param_name in ['z_threshold', 'z_exit_threshold']:
            best_params[param_name] = max(0.1, min(5.0, best_params[param_name]))

best_score = -result_bo.fun  # 取负号，因为返回的是负得分
```

**原理**：
- `result_bo.x`：最佳参数值（数组）
- `result_bo.fun`：最佳得分（负数，因为是最小化）
- 需要取负号得到真实的得分
- 确保参数在合理范围内

### 完整流程图

```
开始贝叶斯优化
  ↓
定义搜索空间（dimensions）
  ↓
定义目标函数（objective）
  ↓
[初始化]
随机采样几个初始点
  ↓
评估初始点
  ↓
建立初始高斯过程模型
  ↓
[迭代 n_calls 次]
  ↓
使用采集函数选择下一个评估点
  ↓
评估该参数组合
  ↓
更新高斯过程模型
  ↓
继续下一次迭代
  ↓
提取最佳参数和得分
  ↓
[稳定性测试]（可选）
  ↓
返回最佳结果
```

---

## 过拟合检测机制

### 什么是过拟合？

过拟合是指参数在训练数据上表现很好，但在新数据上表现较差的现象。

### 稳定性测试原理

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
def test_parameter_stability(self, params: Dict[str, Any], 
                             perturbation_ratio: float = 0.1,
                             num_tests: int = 5) -> Dict[str, Any]:
    """
    测试参数稳定性（过拟合检测）
    在参数附近生成扰动，检查结果是否稳定
    """
    base_result = self.evaluate_params(params, verbose=False)
    base_score = base_result['score']
    
    perturbed_scores = []
    
    for _ in range(num_tests):
        # 生成扰动参数
        perturbed_params = params.copy()
        for param_name, param_value in params.items():
            if param_name in self.param_space:
                space_def = self.param_space[param_name]
                if space_def['type'] == 'int':
                    step = max(1, int(space_def['fine_step'] * perturbation_ratio))
                    perturbed_params[param_name] = param_value + random.randint(-step, step)
                else:
                    step = space_def['fine_step'] * perturbation_ratio
                    perturbed_params[param_name] = param_value + random.uniform(-step, step)
        
        # 评估扰动参数
        perturbed_result = self.evaluate_params(perturbed_params, verbose=False)
        perturbed_scores.append(perturbed_result['score'])
    
    # 计算稳定性指标
    score_std = np.std(perturbed_scores)
    score_mean = np.mean(perturbed_scores)
    score_cv = abs(score_std / score_mean) if score_mean != 0 else float('inf')
    score_drop = (base_score - score_mean) / abs(base_score) if base_score != 0 else 0
    
    stability_result = {
        'base_score': base_score,
        'perturbed_mean_score': score_mean,
        'perturbed_std_score': score_std,
        'score_coefficient_of_variation': score_cv,
        'score_drop_ratio': score_drop,
        'is_stable': score_cv < 0.3 and score_drop < 0.2,  # 稳定性阈值
    }
    
    return stability_result
```

### 测试步骤

1. **评估原始参数**：使用最佳参数进行评估，得到基准得分

2. **生成扰动参数**：
   - 在最佳参数附近生成5个扰动版本
   - 扰动幅度：`fine_step × perturbation_ratio`（默认10%）

3. **评估扰动参数**：对每个扰动参数进行评估

4. **计算稳定性指标**：
   - **变异系数（CV）**：`score_std / score_mean`（得分波动程度）
   - **得分下降比例**：`(base_score - mean_score) / base_score`（扰动后得分下降程度）

5. **判断稳定性**：
   - 如果 `CV < 0.3` 且 `score_drop < 0.2`，认为参数稳定
   - 否则，可能过拟合

### 稳定性判断标准

**稳定参数**：
- 变异系数 < 0.3：扰动后得分波动小
- 得分下降比例 < 0.2：扰动后得分下降不明显

**不稳定参数（可能过拟合）**：
- 变异系数 ≥ 0.3：扰动后得分波动大
- 得分下降比例 ≥ 0.2：扰动后得分明显下降

---

## 三种方法对比

### 计算效率对比

| 方法 | 计算量 | 时间复杂度 | 适用场景 |
|------|--------|-----------|---------|
| **网格搜索** | 高（全面搜索） | O(n^m) | 参数空间小（<1000组合） |
| **随机搜索** | 中（可控） | O(n) | 参数空间大，计算资源有限 |
| **贝叶斯优化** | 中（智能搜索） | O(n) | 参数空间大，评估成本高 |

其中：
- n：迭代次数/评估次数
- m：参数数量

### 搜索策略对比

| 方法 | 搜索策略 | 是否利用历史信息 | 搜索效率 |
|------|---------|----------------|---------|
| **网格搜索** | 穷举搜索 | 否 | 低（但全面） |
| **随机搜索** | 随机采样 | 否 | 中 |
| **贝叶斯优化** | 智能引导 | 是（使用概率模型） | 高 |

### 结果质量对比

| 方法 | 找到全局最优解的概率 | 结果稳定性 | 过拟合风险 |
|------|-------------------|-----------|-----------|
| **网格搜索** | 高（如果参数空间覆盖完整） | 高 | 中 |
| **随机搜索** | 中（取决于采样次数） | 中 | 中 |
| **贝叶斯优化** | 高（智能搜索） | 中 | 中 |

### 使用建议

1. **参数空间小（<5个参数，每个<10个候选值）**：
   - 推荐：**网格搜索**
   - 原因：可以全面搜索，不会遗漏最优解

2. **参数空间大，计算资源有限**：
   - 推荐：**随机搜索**
   - 原因：计算量可控，结果可接受

3. **参数空间大，评估成本高（每次回测时间长）**：
   - 推荐：**贝叶斯优化**
   - 原因：智能搜索，通常用更少的评估次数找到更好的解

---

## 优化结果输出

### 结果字典结构

**代码**：
```python:cointegration_test_windows_optimization_arima_garch.py
return {
    'method': 'grid_search',  # 或 'random_search', 'bayesian_optimization'
    'best_params': best_result['params'],  # 最佳参数
    'best_score': best_score,  # 最佳得分
    'best_result': best_result,  # 完整的最佳结果
    'total_evaluations': len(self.evaluation_history)  # 总评估次数
}
```

### best_result 包含的信息

```python
{
    'params': {...},  # 参数组合
    'score': 1.234,  # 得分
    'total_return': 15.6,  # 总收益率（%）
    'sharpe_ratio': 1.234,  # 夏普比率
    'max_drawdown_pct': 5.2,  # 最大回撤（%）
    'profit_loss_ratio': 2.5,  # 盈亏比
    'total_trades': 50,  # 总交易次数
    'profitable_trades': 30,  # 盈利交易次数
    'final_capital': 11560.0,  # 最终资金
    'stability': {...}  # 稳定性测试结果（如果执行了）
}
```

---

## 代码文件位置

所有优化相关代码位于：
- `cointegration_test_windows_optimization_arima_garch.py`：第1965-2570行

---

**文档版本**：v1.0  
**最后更新**：2024年  
**作者**：量化交易系统开发团队

